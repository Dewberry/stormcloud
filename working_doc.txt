.env
	- ms creds
	- s3 creds
	- zarr s3 bucket

overall:
	user inputs:
		- output zip
		- s3 dss write prefix
		- mean filter
		- limit
		- top by year
		- year filter
		- watershed name
		- transposition domain name
		- output resolution in meters
		- data variables
	- define variables to extract as temperature if included in data variables
	- queries meilisearch database to get top 440 scripts
	- initializes list of s3 uris for DSS record metadata as empty list
	- for each storm document in results:
		- get:
			- start date
			- end date
			- storm x
			- storm y
			- overall rank
			- rank within year (if using)
			- precipitation dss path
			- last modification
			- watershed geojson s3 path
			- transposition geojson s3 path
		- if output resolution is 4000 (resolution used in SST runs):
			- reconstruct precipitation dss s3 path using metadata
			- reconstruct precipitation JSON document
			- construct metadata document including watershed, transposition domain, start date, end date, storm x, storm y, rank info, and last modification datetime, and example pathname and save to s3
		- else:
			- add precipitation to data variables to extract
		- use start date, end date, watershed geojson s3 path, and transposition domain geojson s3 path to extract dss for variables to extract
		- produce metadata package for generated dss and save to s3
		- append s3 uri for metadata to list
	- open grid writer
	- for each dss metadata package in list:
		- download dss file
		- write to grid file
	- zip directory containing grid file and dss subdirectory containing temperature and precipitation dss records

scripts:
	- meilisearch query script (already exists)
	- script to get metadata for an identified storm datetime, watershed, and transposition domain and save to s3 path (requires modification of existing script)
	- script to use start time, end time, watershed, transposition region, output resolution, and data variables of interest to generate a dss and save it to s3 with a specified prefix in a given bucket with a predictable file name pattern, then produce a metadata file and save out to s3 as well (requires modification of exisiting script)
	- script to take in list of s3 paths of job metadata (including watershed, rank, last date of modification, DSS s3 path) and create a GRID package from these inputs (new script)